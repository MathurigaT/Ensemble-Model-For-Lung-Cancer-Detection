{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bvFE0zwzDABW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894c40af-32d5-4266-eef4-f85b5ba68632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OyusCCvkDbWg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data directory\n",
        "data_dir = \"drive/MyDrive/NewLungData\"\n",
        "\n",
        "# Define mean and standard deviation for normalization\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "# Define image size\n",
        "image_size = (256, 256)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Define data generators for train, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'train'),\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'val'),\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'test'),\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8vr82YzD7eh",
        "outputId": "55ee0a88-7b7b-4ed0-cb3d-45ca94750cee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5641 images belonging to 2 classes.\n",
            "Found 1297 images belonging to 2 classes.\n",
            "Found 1622 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load a saved model\n",
        "def load_model(model_load_path):\n",
        "    loaded_model = tf.keras.models.load_model(model_load_path)\n",
        "    return loaded_model"
      ],
      "metadata": {
        "id": "Z4usOcgUD82R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for validation\n",
        "def validate_model(model, val_dataset):\n",
        "    # Define optimizer and learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    # Compile the model (if not compiled previously)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Evaluate the model on the validation dataset\n",
        "    val_loss, val_accuracy = model.evaluate(val_dataset)\n",
        "\n",
        "    # Make predictions on the validation dataset\n",
        "    val_predictions = model.predict(val_dataset)\n",
        "    val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "    # Get true labels from the validation dataset\n",
        "    y_true = val_dataset.classes\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    report = classification_report(y_true, val_pred_labels, target_names=val_dataset.class_indices)\n",
        "\n",
        "    print(\"Validation Loss:\", val_loss)\n",
        "    print(\"Validation Accuracy:\", val_accuracy)\n",
        "    print(\"Validation Classification Report:\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "AC_JuRcnet_F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for testing\n",
        "def test_model(model, test_dataset):\n",
        "    # Define optimizer and learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    # Compile the model (if not compiled previously)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "     # Evaluate the model on the test dataset\n",
        "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "    # Make predictions on the test dataset\n",
        "    test_predictions = model.predict(test_dataset)\n",
        "    test_pred_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "    # Get true labels from the validation dataset\n",
        "    y_true = test_dataset.classes\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    report = classification_report(y_true, test_pred_labels, target_names=test_dataset.class_indices)\n",
        "\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"Test Accuracy:\", test_accuracy)\n",
        "    print(\"Test Classification Report:\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "YSQ0ENxUe8h2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DenseNet169 model which is pretrained from 450 layer\n",
        "Dense169_model=load_model(\"drive/MyDrive/NewLungData/tenserflowModels/DenseNet169_fn_450\")"
      ],
      "metadata": {
        "id": "KmlmUWzTFNNB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ResNet101 model which is pretrained from 200 layer\n",
        "resnet_101_model=load_model(\"drive/MyDrive/NewLungData/tenserflowModels/ResNet101_fn_200\")"
      ],
      "metadata": {
        "id": "2CWEgDtGFbXY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 model which is pretrained from 200 layer\n",
        "vgg16_model=load_model(\"drive/MyDrive/NewLungData/tenserflowModels/vgg16_fn_5\")"
      ],
      "metadata": {
        "id": "KiJglCe_Bo4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the model using the name attribute\n",
        "Dense169_model._name = 'Dense169_model'\n",
        "Dense169_model.summary()"
      ],
      "metadata": {
        "id": "_EeRX_xtXzVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_101_model._name=\"resnet101_model\"\n",
        "resnet_101_model.summary()"
      ],
      "metadata": {
        "id": "V_vr5m2yaAdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model._name=\"vgg16_model\"\n",
        "vgg16_model.summary()"
      ],
      "metadata": {
        "id": "y5tqPU8UaFzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Technique 01: Ranked base with weights"
      ],
      "metadata": {
        "id": "wMsL4oB_LGXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define rankings of accuracy and overfitting for each model\n",
        "accuracy = {'VGG16': 0.942046880722045, 'DenseNet': 0.92601728439331, 'ResNet': 0.900123298168182}\n",
        "\n",
        "# Define rankings of accuracy for each model\n",
        "accuracy_rankings = {'VGG16': 3, 'DenseNet': 2, 'ResNet': 1}\n",
        "\n",
        "# Calculate total accuracy\n",
        "total_accuracy = sum(accuracy_rankings.values())\n",
        "\n",
        "# Normalize each model accuracy\n",
        "normalized_accuracy_rankings = {model: accuracy / total_accuracy for model, accuracy in accuracy_rankings.items()}\n",
        "\n",
        "print(\"Normalized Accuracy Rankings:\", normalized_accuracy_rankings)\n",
        "\n",
        "# Define overfitting rankings\n",
        "overfit_rankings = {'DenseNet': 3, 'ResNet': 2, 'VGG16': 1}\n",
        "\n",
        "# Convert overfitting rankings to array\n",
        "ranks = np.array(list(overfit_rankings.values()))\n",
        "\n",
        "# Normalize overfitting ranks\n",
        "normalized_ranks = ranks / np.sum(ranks)\n",
        "\n",
        "# Update overfitting rankings with normalized ranks\n",
        "normalized_overfit_rankings = {model: rank for model, rank in zip(overfit_rankings.keys(), normalized_ranks)}\n",
        "\n",
        "print(\"Original Overfitting Rankings:\", overfit_rankings)\n",
        "print(\"Normalized Overfitting Rankings:\", normalized_overfit_rankings)\n",
        "\n",
        "# Define weights for accuracy and overfitting\n",
        "accuracy_weight = 0.5\n",
        "overfitting_weight = 0.5\n",
        "\n",
        "# Multiply accuracy rankings by accuracy weight\n",
        "weighted_accuracy_rankings = {model: accuracy * accuracy_weight for model, accuracy in normalized_accuracy_rankings.items()}\n",
        "\n",
        "# Multiply normalized overfitting rankings by overfitting weight\n",
        "weighted_overfitting_rankings = {model: overfit * overfitting_weight for model, overfit in normalized_overfit_rankings.items()}\n",
        "\n",
        "# Combine the weighted rankings\n",
        "combined_rankings = {}\n",
        "for model in accuracy_rankings.keys():\n",
        "    combined_rankings[model] = weighted_accuracy_rankings[model] + weighted_overfitting_rankings[model]\n",
        "\n",
        "# Sort models based on combined rankings\n",
        "sorted_models = sorted(combined_rankings.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Combined Rankings:\", sorted_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woRuOA766TkX",
        "outputId": "cc7359a8-a171-45c6-c6e0-6c7a6041f65c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Overfitting Rankings: {'DenseNet': 3, 'ResNet': 2, 'VGG16': 1}\n",
            "Normalized Overfitting Rankings: {'DenseNet': 0.5, 'ResNet': 0.3333333333333333, 'VGG16': 0.16666666666666666}\n",
            "Combined Rankings: [('DenseNet', 0.713008642196655), ('ResNet', 0.6167283157507577), ('VGG16', 0.5543567736943559)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the validation data generator for ensemble model\n",
        "def validation_ensemble_data_generator(val_generator):\n",
        "    for x_batch, y_batch in val_generator:\n",
        "        # Resize images for Dense169 and ResNet-101 (256x256)\n",
        "        resized_x_batch_256 = tf.image.resize(x_batch, (256, 256))\n",
        "        # Resize images for VGG16 (224x224)\n",
        "        resized_x_batch_224 = tf.image.resize(x_batch, (224, 224))\n",
        "        yield ([resized_x_batch_256, resized_x_batch_224], y_batch)"
      ],
      "metadata": {
        "id": "3tG9yLKJc5uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the test data generator for ensemble model\n",
        "def test_ensemble_data_generator(test_generator):\n",
        "    for x_batch, y_batch in test_generator:\n",
        "        # Resize images for Dense169 and ResNet-101 (256x256)\n",
        "        resized_x_batch_256 = tf.image.resize(x_batch, (256, 256))\n",
        "        # Resize images for VGG16 (224x224)\n",
        "        resized_x_batch_224 = tf.image.resize(x_batch, (224, 224))\n",
        "        yield ([resized_x_batch_256, resized_x_batch_224], y_batch)\n"
      ],
      "metadata": {
        "id": "Vp4gTS3VdF6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for ensemble model validation\n",
        "def validate_ensemble_model(model, val_dataset, steps):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    val_loss, val_accuracy = model.evaluate(val_dataset, steps=steps)\n",
        "\n",
        "    print(\"Validation Loss:\", val_loss)\n",
        "    print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "p4gZ97g0c9Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for ensemble model validation\n",
        "def test_rank_based_model(model, val_dataset, steps):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    test_loss, test_accuracy = model.evaluate(val_dataset, steps=steps)\n",
        "\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "fBereyt4dYWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Input layer\n",
        "image_size = (256, 256, 3)\n",
        "inputs = tf.keras.Input(shape=image_size)\n",
        "\n",
        "# Set the loaded models to non-trainable\n",
        "Dense169_model.trainable = False\n",
        "resnet_101_model.trainable = False\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "# Get the outputs of the loaded models\n",
        "dense_output = Dense169_model(inputs)\n",
        "resnet_output = resnet_101_model(inputs)\n",
        "\n",
        "# VGG16 model requires different image size as input, hence input size needs to be changed to get the output\n",
        "vgg_image_size = (224, 224, 3)\n",
        "vgg_inputs = tf.keras.Input(shape=vgg_image_size)\n",
        "vgg16_output = vgg16_model(vgg_inputs)\n",
        "\n",
        "# Assign calculated combined ranking score for each model\n",
        "dense169_cr_score = 0.713008642196655\n",
        "resnet101_cr_score = 0.6167283157507577\n",
        "vgg16_cr_score = 0.5543567736943559\n",
        "\n",
        "# Calculate weights based on combined ranking and tanh function\n",
        "cr_models = np.array([dense169_cr_score, resnet101_cr_score,vgg16_cr_score])\n",
        "ranked_indices = np.argsort(cr_models)[::-1]\n",
        "weights = tf.math.tanh(1 / (ranked_indices + 1))\n",
        "\n",
        "# Normalize the weights\n",
        "normalized_weights = weights / tf.reduce_sum(weights)\n",
        "\n",
        "dense169_weight = normalized_weights[0].numpy()\n",
        "resnet101_weight = normalized_weights[1].numpy()\n",
        "vgg16_weight= normalized_weights[2].numpy()\n",
        "\n",
        "# Combine the outputs with weighted voting\n",
        "weighted_output = tf.keras.layers.Lambda(lambda x: (x[0] * dense169_weight\n",
        "                                                    + x[1] * resnet101_weight\n",
        "                                                    + x[2] * vgg16_weight)\n",
        "                      / (dense169_weight + resnet101_weight + vgg16_weight))\n",
        "                      ([dense_output, resnet_output, vgg16_output])\n",
        "\n",
        "# Round the weighted output to obtain binary predictions\n",
        "voted_output = tf.keras.layers.Lambda(lambda x: tf.math.round(x))(weighted_output)\n",
        "\n",
        "# Define the ensemble model\n",
        "ensemble_model_tan = tf.keras.Model(inputs=[inputs, vgg_inputs], outputs=voted_output, name='ensemble_model_rank')\n",
        "\n",
        "# Calculate the total number of samples in the validation set\n",
        "total_val_samples = len(val_generator)\n",
        "# Calculate the total number of steps based on the batch size\n",
        "val_steps = total_val_samples // batch_size * 10\n",
        "\n",
        "# Validate the ensemble model\n",
        "validate_ensemble_model(ensemble_model_tan, validation_ensemble_data_generator(val_generator), val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI7JCe0J9nGW",
        "outputId": "ed774fa2-9bbe-4772-a4fb-2f4622fe180e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 11s 314ms/step - loss: 0.4228 - accuracy: 0.9738\n",
            "Validation Loss: 0.42276981472969055\n",
            "Validation Accuracy: 0.9737704992294312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of samples in the validation set\n",
        "total_test_samples = len(test_generator)\n",
        "# Calculate the total number of steps based on the batch size\n",
        "test_steps = total_test_samples // batch_size * 10\n",
        "\n",
        " # Test the ensemble model\n",
        "test_rank_based_model(ensemble_model_tan, test_ensemble_data_generator(test_generator), test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztB7H0w1F_ac",
        "outputId": "b820b156-fdd1-470d-f294-223faf74fa6e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 130s 14s/step - loss: 0.9570 - accuracy: 0.9406\n",
            "Test Loss: 0.9570119976997375\n",
            "Test Accuracy: 0.940625011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Technique 02: Average"
      ],
      "metadata": {
        "id": "FWqXkc30cQgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Input layer\n",
        "image_size = (256, 256, 3)\n",
        "inputs = tf.keras.Input(shape=image_size)\n",
        "\n",
        "# Set the loaded models to non-trainable\n",
        "Dense169_model.trainable = False\n",
        "resnet_101_model.trainable = False\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "# Get the outputs of the loaded models\n",
        "dense_output = Dense169_model(inputs)\n",
        "resnet_output = resnet_101_model(inputs)\n",
        "\n",
        "# VGG16 model requires different image size as input, hence input size needs to be changed to get the output\n",
        "vgg_image_size = (224, 224, 3)\n",
        "vgg_inputs = tf.keras.Input(shape=vgg_image_size)\n",
        "vgg16_output = vgg16_model(vgg_inputs)\n",
        "\n",
        "# Average the predictions\n",
        "averaged_output = tf.keras.layers.Average()([dense_output, resnet_output, vgg16_output])\n",
        "\n",
        "# Define the ensemble model\n",
        "ensemble_model_average = tf.keras.Model(inputs=[inputs, vgg_inputs], outputs=averaged_output, name='ensemble_model_average')\n",
        "\n",
        "# Validate the ensemble model\n",
        "validate_ensemble_model(ensemble_model_average, validation_ensemble_data_generator(val_generator), val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxs3hZURH3Ij",
        "outputId": "5a2aeb8b-e79a-44a4-a76e-9463f697c1f2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 11s 335ms/step - loss: 0.2050 - accuracy: 0.9625\n",
            "Validation Loss: 0.20499460399150848\n",
            "Validation Accuracy: 0.9624999761581421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the ensemble model\n",
        "test_rank_based_model(ensemble_model_average, test_ensemble_data_generator(test_generator), test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsnjuOBaIQpB",
        "outputId": "496b2223-48c5-4b4e-a038-078f62467862"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 88s 9s/step - loss: 0.2809 - accuracy: 0.9290\n",
            "Test Loss: 0.28086769580841064\n",
            "Test Accuracy: 0.9290322661399841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Technique 03: Hard Voting"
      ],
      "metadata": {
        "id": "rwdMCCOPfn0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Input layer\n",
        "image_size = (256, 256, 3)\n",
        "inputs = tf.keras.Input(shape=image_size)\n",
        "\n",
        "# Set the loaded models to non-trainable\n",
        "Dense169_model.trainable = False\n",
        "resnet_101_model.trainable = False\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "# Get the outputs of the loaded models\n",
        "dense_output = Dense169_model(inputs)\n",
        "resnet_output = resnet_101_model(inputs)\n",
        "\n",
        "# VGG16 model requires different image size as input, hence input size needs to be changed to get the output\n",
        "vgg_image_size = (224, 224, 3)\n",
        "vgg_inputs = tf.keras.Input(shape=vgg_image_size)\n",
        "vgg16_output = vgg16_model(vgg_inputs)\n",
        "\n",
        "voted_output = tf.keras.layers.Lambda(lambda x: tf.math.round(tf.reduce_mean(x, axis=0)))([dense_output, resnet_output])\n",
        "\n",
        "# Define the ensemble model\n",
        "ensemble_model_hard_voting = tf.keras.Model(inputs=[inputs, vgg_inputs], outputs=voted_output, name='ensemble_model_voting')\n",
        "\n",
        "# Validate the ensemble model\n",
        "validate_ensemble_model(ensemble_model_hard_voting, validation_ensemble_data_generator(val_generator), val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3jgJAhkI9s9",
        "outputId": "7ef408ec-68d0-4435-9625-361efd2a6869"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 12s 241ms/step - loss: 1.1585 - accuracy: 0.9281\n",
            "Validation Loss: 1.158488154411316\n",
            "Validation Accuracy: 0.9281250238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the ensemble model\n",
        "test_rank_based_model(ensemble_model_hard_voting, test_ensemble_data_generator(test_generator), test_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3v2u8ZcJSb4",
        "outputId": "bb8752ed-a07f-402c-f40d-7c0b724ba579"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 11s 336ms/step - loss: 0.2182 - accuracy: 0.9531\n",
            "Test Loss: 0.21819451451301575\n",
            "Test Accuracy: 0.953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Technique 04: Stacking-Random Forest Classifier"
      ],
      "metadata": {
        "id": "PjvuueQ9fwGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the base models\n",
        "Dense169_model.trainable = False\n",
        "resnet_101_model.trainable = False\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "# Get output of train, validation and test data set\n",
        "dense_output_train = Dense169_model.predict(train_generator)\n",
        "dense_output_val = Dense169_model.predict(val_generator)\n",
        "dense_output_test = Dense169_model.predict(test_generator)\n",
        "\n",
        "resnet_output_train = resnet_101_model.predict(train_generator)\n",
        "resnet_output_val = resnet_101_model.predict(val_generator)\n",
        "resnet_output_test = resnet_101_model.predict(test_generator)\n",
        "\n",
        "\n",
        "vgg_output_train = vgg16_model.predict(train_generator)\n",
        "vgg_output_val = vgg16_model.predict(val_generator)\n",
        "vgg_output_test = vgg16_model.predict(test_generator)\n",
        "\n",
        "# Concatenate the outputs to get as features\n",
        "X_train_combined = np.concatenate((dense_output_train, resnet_output_train,vgg_output_train), axis=1)\n",
        "X_val_combined = np.concatenate((dense_output_val, resnet_output_val,vgg_output_val), axis=1)\n",
        "X_test_combined = np.concatenate((dense_output_test, resnet_output_test,vgg_output_test), axis=1)\n",
        "\n",
        "# Define the labels\n",
        "y_train = train_generator.classes\n",
        "y_val = val_generator.classes\n",
        "y_test = test_generator.classes\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_combined, y_train)\n",
        "\n",
        "# Validation\n",
        "val_predictions = rf_classifier.predict(X_val_combined)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "print(\"Validation Classification Report:\")\n",
        "print(classification_report(y_val, val_predictions))\n",
        "\n",
        "# Testing\n",
        "test_predictions = rf_classifier.predict(X_test_combined)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alq9nSC-J_53",
        "outputId": "7ead072d-8f54-4606-bef8-b8d52b5c7466"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "177/177 [==============================] - 2082s 12s/step\n",
            "41/41 [==============================] - 4s 95ms/step\n",
            "51/51 [==============================] - 5s 89ms/step\n",
            "177/177 [==============================] - 24s 124ms/step\n",
            "41/41 [==============================] - 5s 120ms/step\n",
            "51/51 [==============================] - 6s 121ms/step\n",
            "177/177 [==============================] - 30s 137ms/step\n",
            "41/41 [==============================] - 8s 206ms/step\n",
            "51/51 [==============================] - 10s 205ms/step\n",
            "Validation Accuracy: 0.8033924441017734\n",
            "Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89      1073\n",
            "           1       0.28      0.08      0.13       224\n",
            "\n",
            "    accuracy                           0.80      1297\n",
            "   macro avg       0.55      0.52      0.51      1297\n",
            "weighted avg       0.74      0.80      0.76      1297\n",
            "\n",
            "Test Accuracy: 0.7916152897657214\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88      1340\n",
            "           1       0.18      0.06      0.09       282\n",
            "\n",
            "    accuracy                           0.79      1622\n",
            "   macro avg       0.50      0.50      0.48      1622\n",
            "weighted avg       0.71      0.79      0.74      1622\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Technique 05: Stacking-Support Vector Classification"
      ],
      "metadata": {
        "id": "VeqN5jMrg-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train the base models\n",
        "Dense169_model.trainable = False\n",
        "resnet_101_model.trainable = False\n",
        "vgg16_model.trainable = False\n",
        "\n",
        "dense_output_train = Dense169_model.predict(train_generator)\n",
        "dense_output_val = Dense169_model.predict(val_generator)\n",
        "dense_output_test = Dense169_model.predict(test_generator)\n",
        "\n",
        "resnet_output_train = resnet_101_model.predict(train_generator)\n",
        "resnet_output_val = resnet_101_model.predict(val_generator)\n",
        "resnet_output_test = resnet_101_model.predict(test_generator)\n",
        "\n",
        "\n",
        "vgg_output_train = vgg16_model.predict(train_generator)\n",
        "vgg_output_val = vgg16_model.predict(val_generator)\n",
        "vgg_output_test = vgg16_model.predict(test_generator)\n",
        "\n",
        "# Concatenate the outputs to use as features\n",
        "X_train_combined = np.concatenate((dense_output_train, resnet_output_train,vgg_output_train), axis=1)\n",
        "X_val_combined = np.concatenate((dense_output_val, resnet_output_val,vgg_output_val), axis=1)\n",
        "X_test_combined = np.concatenate((dense_output_test, resnet_output_test,vgg_output_test), axis=1)\n",
        "\n",
        "# Define the labels\n",
        "y_train = train_generator.classes\n",
        "y_val = val_generator.classes\n",
        "y_test = test_generator.classes\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
        "svm_classifier.fit(X_train_combined, y_train)\n",
        "\n",
        "# Validation\n",
        "val_predictions = svm_classifier.predict(X_val_combined)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "print(\"Validation Classification Report:\")\n",
        "print(classification_report(y_val, val_predictions))\n",
        "\n",
        "# Testing\n",
        "test_predictions = rf_classifier.predict(X_test_combined)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2erhZMRLDUo",
        "outputId": "7518e59b-461b-4961-86b3-34399b96bcc3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "177/177 [==============================] - 17s 95ms/step\n",
            "41/41 [==============================] - 4s 89ms/step\n",
            "51/51 [==============================] - 5s 95ms/step\n",
            "177/177 [==============================] - 22s 122ms/step\n",
            "41/41 [==============================] - 5s 121ms/step\n",
            "51/51 [==============================] - 6s 120ms/step\n",
            "177/177 [==============================] - 22s 126ms/step\n",
            "41/41 [==============================] - 5s 125ms/step\n",
            "51/51 [==============================] - 6s 126ms/step\n",
            "Validation Accuracy: 0.8272937548188126\n",
            "Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      1073\n",
            "           1       0.00      0.00      0.00       224\n",
            "\n",
            "    accuracy                           0.83      1297\n",
            "   macro avg       0.41      0.50      0.45      1297\n",
            "weighted avg       0.68      0.83      0.75      1297\n",
            "\n",
            "Test Accuracy: 0.7940813810110974\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88      1340\n",
            "           1       0.18      0.05      0.08       282\n",
            "\n",
            "    accuracy                           0.79      1622\n",
            "   macro avg       0.50      0.50      0.48      1622\n",
            "weighted avg       0.71      0.79      0.74      1622\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}